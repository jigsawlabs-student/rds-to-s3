{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exceptional-column",
   "metadata": {},
   "source": [
    "# RDS to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-chicago",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-consequence",
   "metadata": {},
   "source": [
    "In the last set of lessons, we saw how we could copy data from a CSV file in S3, and move it to our database in redshift.  But remember that our data does not start out in S3.  Instead, it generally starts from our RDS database, and from there we move the data over to a CSV file in S3 to ultimately move over to redshift. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-money",
   "metadata": {},
   "source": [
    "Let's take another look at the diagram illustrating our data flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-helen",
   "metadata": {},
   "source": [
    "<img src=\"./rds_ec2_s3.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-conspiracy",
   "metadata": {},
   "source": [
    "As we can see, to coordinate the flow of data from RDS to S3, we use an ETL server to move our data from RDS to S3, and ultimately to redshift.\n",
    "\n",
    "In this lesson, we'll use our local laptop to copy our data first to our laptop and, from there, over to our S3 buckets.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-riverside",
   "metadata": {},
   "source": [
    "### Our Current Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-southeast",
   "metadata": {},
   "source": [
    "Now, let's imagine that we have already setup our Amazon RDS instance.  And that in our RDS, we have setup our `foursquare-api` database, which has the following tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-lender",
   "metadata": {},
   "source": [
    "> <img src=\"./display_tables.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-course",
   "metadata": {},
   "source": [
    "### Exporting Data to Our Local Computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-startup",
   "metadata": {},
   "source": [
    "So now, let's begin by exporting data from our postgres database to our local ETL machine.  We'll export data from our states table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-network",
   "metadata": {},
   "source": [
    "> <img src=\"./display-select-states.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-highlight",
   "metadata": {},
   "source": [
    "And we export our data with the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-pressing",
   "metadata": {},
   "source": [
    "```bash\n",
    "psql -U postgres --host foursquare-flask-api.cbdkozm37vkd.us-east-1.rds.amazonaws.com -c \"\\copy (Select * From states) To 'states.csv' With CSV;\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-purpose",
   "metadata": {},
   "source": [
    "> Other optional arguments can be in the [postgres documentation for copy](https://www.postgresql.org/docs/9.2/sql-copy.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-pasta",
   "metadata": {},
   "source": [
    "### Moving to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-projection",
   "metadata": {},
   "source": [
    "Now if we take another look at our diagram, at this point, we have just exported data from our RDS instance over to our local machine with the postgres command of `\\copy (Select * From states) To 'states.csv' With CSV;`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-blind",
   "metadata": {},
   "source": [
    "<img src=\"./rds_ec2_s3.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-active",
   "metadata": {},
   "source": [
    "The next step is to then export this CSV file over to our S3 file storage.  We can do export our data through a one line statement in the AWS command line, but first, we'll need to make sure we have the S3 proper permissions to upload this file to our bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-damages",
   "metadata": {},
   "source": [
    "Let's navigate to the s3 dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-composer",
   "metadata": {},
   "source": [
    "> <img src=\"./s3-other-dash.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-closure",
   "metadata": {},
   "source": [
    "And select the name of the bucket that you would like to upload the csv files to.  Here, it's `jigsaw-sample-data`.  Then we'll select permissions, and from there will enable public access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-socket",
   "metadata": {},
   "source": [
    "> <img src=\"./sample-data-public.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-border",
   "metadata": {},
   "source": [
    "> We can eventually change this to only allow access from a specific IP address or EC2 instance, but for now this is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-snowboard",
   "metadata": {},
   "source": [
    "From there, the next step is to paste in the following as the bucket policy.\n",
    "\n",
    "> The only information, we may need to alter is the resource -- change it to the name of your bucket, if it's different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-london",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PublicRead\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": \"*\",\n",
    "            \"Action\": [\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:PutObjectAcl\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:GetObjectAcl\",\n",
    "                \"s3:DeleteObject\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::jigsaw-sample-data/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-fantasy",
   "metadata": {},
   "source": [
    "So it should look like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-system",
   "metadata": {},
   "source": [
    "> <img src=\"./bucket-policy.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-rubber",
   "metadata": {},
   "source": [
    "> FYI: `acl` stands for access control list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-living",
   "metadata": {},
   "source": [
    "Now that the permissions are properly setup, we can move to the command to take our data from our local machine and upload it into into our bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-burner",
   "metadata": {},
   "source": [
    "<img src=\"./states-csv.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-tablet",
   "metadata": {},
   "source": [
    "> `aws s3 cp states.csv s3://jigsaw-sample-data/states.csv --acl public-read`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-malaysia",
   "metadata": {},
   "source": [
    "And then we can see that we have the `states.csv` file located in the specified bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-roots",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-burton",
   "metadata": {},
   "source": [
    "In this lesson, we saw the procedure for copying our data from an RDS instance, to a CSV file on an external computer, and then uploading the CSV file to S3.  \n",
    "\n",
    "To copy the data from our RDS instance to the ETL server, we ran the following:\n",
    "\n",
    "```bash\n",
    "psql -U postgres --host foursquare-flask-api.cbdkozm37vkd.us-east-1.rds.amazonaws.com -c \"\\copy (Select * From states) To 'states.csv' With CSV;\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-element",
   "metadata": {},
   "source": [
    "And this created a new `states.csv` file on our computer with data from our RDS instance.\n",
    "\n",
    "Then came uploading this file to our `s3` bucket.  First, we set permissions on our S3 bucket to enable public access, and set a bucket policy that gave `put` access to add additional objects to the bucket.  After the permissions were set up, we were able to upload our csv file into the bucket with the following: \n",
    "\n",
    "`aws s3 cp states.csv s3://jigsaw-sample-data/states.csv --acl public-read`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-public",
   "metadata": {},
   "source": [
    "### Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-individual",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[S3 tutorial](http://boto.cloudhackers.com/en/latest/s3_tut.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-louisiana",
   "metadata": {},
   "source": [
    "[Good tutorial RDS to S3](https://www.sqlshack.com/integrating-aws-s3-buckets-with-aws-rds-sql-server/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
