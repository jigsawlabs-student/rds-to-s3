{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "instant-better",
   "metadata": {},
   "source": [
    "# RDS to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-sunrise",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-hypothetical",
   "metadata": {},
   "source": [
    "In the last set of lessons, we saw copy data from a CSV file in S3, and move it to our database in redshift.  But remember that our data does not start out in S3.  Instead, it will generally start from our RDS database, and from there we'll move the data over to a CSV file in S3 to ultimately move over to redshift. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-standing",
   "metadata": {},
   "source": [
    "Let's take another look at the diagram illustrating our data flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-underground",
   "metadata": {},
   "source": [
    "<img src=\"./rds_ec2_s3.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-irish",
   "metadata": {},
   "source": [
    "As we can see, to coordinate the flow of data from RDS to S3, we use an ETL server to perform commands moving data from RDS to S3, and ultimately to redshift.\n",
    "\n",
    "In this lesson, we'll use our local laptop to copy our data first to our laptop and, from there, over to our S3 buckets.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-powder",
   "metadata": {},
   "source": [
    "### Our Current Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-addiction",
   "metadata": {},
   "source": [
    "Now, let's imagine that we already have our RDS instance setup.  We are using our foursquare-api database, which has the following tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-penetration",
   "metadata": {},
   "source": [
    "> <img src=\"./display_tables.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-consideration",
   "metadata": {},
   "source": [
    "### Exporting Data to Our Local Computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-basketball",
   "metadata": {},
   "source": [
    "Imagine that we want to begin by exporting data from our postgres database to our local machine.  Let's choose the informatioon in the states table as data to move."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-measure",
   "metadata": {},
   "source": [
    "> <img src=\"./display-select-states.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-stream",
   "metadata": {},
   "source": [
    "Now we can export information from this table onto our local computer as a CSV with the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-canada",
   "metadata": {},
   "source": [
    "```bash\n",
    "psql -U postgres --host foursquare-flask-api.cbdkozm37vkd.us-east-1.rds.amazonaws.com -c \"\\copy (Select * From states) To 'states.csv' With CSV;\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-junction",
   "metadata": {},
   "source": [
    "So here we use the `\\copy` command to `SELECT` information from the proper data table and then specify the file that we want to load it into.  The `WITH` clause is where we place any optional arguments like here the format of CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-front",
   "metadata": {},
   "source": [
    "> Other optional arguments can be in the [postgres documentation for copy](https://www.postgresql.org/docs/9.2/sql-copy.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-following",
   "metadata": {},
   "source": [
    "### Moving to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-prison",
   "metadata": {},
   "source": [
    "Now if we take another look at our diagram, at this point, we have just exported data from our RDS instance over to our local machine with the postgres command of `\\copy (Select * From states) To 'states.csv' With CSV;`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-voltage",
   "metadata": {},
   "source": [
    "<img src=\"./rds_ec2_s3.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-parking",
   "metadata": {},
   "source": [
    "The next step is to then export this CSV file over to our S3 file storage.  We can do export our data through a one line statement in the AWS command line, but first, we'll need to make sure we have the proper permissions to perform this upload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-screw",
   "metadata": {},
   "source": [
    "To do so, navigate to the s3 dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-integer",
   "metadata": {},
   "source": [
    "> <img src=\"./s3-other-dash.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-audit",
   "metadata": {},
   "source": [
    "And select the name of the bucket that you would like to upload the csv files to.  Here, it's `jigsaw-sample-data`.  Then we'll select permissions, and the first step we can do is to enable public access.\n",
    "\n",
    "> We can eventually change this to only allow access from a specific IP address or EC2 instance, but for now this is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-referral",
   "metadata": {},
   "source": [
    "> <img src=\"./sample-data-public.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-lobby",
   "metadata": {},
   "source": [
    "From there, the next step is to paste in the following as the bucket policy.\n",
    "\n",
    "> The only information, we may need to alter is the resource -- change it to the name of your bucket, if different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-ensemble",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PublicRead\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": \"*\",\n",
    "            \"Action\": [\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:PutObjectAcl\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:GetObjectAcl\",\n",
    "                \"s3:DeleteObject\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::jigsaw-sample-data/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-dover",
   "metadata": {},
   "source": [
    "> <img src=\"./bucket-policy.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-retro",
   "metadata": {},
   "source": [
    "> FYI: `acl` stands for access control list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-course",
   "metadata": {},
   "source": [
    "Now that the permissions are properly setup, we can move to the command to take our data from our local machine and upload it into into our bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-crystal",
   "metadata": {},
   "source": [
    "<img src=\"./states-csv.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-choir",
   "metadata": {},
   "source": [
    "> `aws s3 cp states.csv s3://jigsaw-sample-data/states.csv --acl public-read`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-milton",
   "metadata": {},
   "source": [
    "And then we can see that we have the `states.csv` file located in the specified bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-bunch",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-stranger",
   "metadata": {},
   "source": [
    "In this lesson, we saw the procedure for copying our data from an RDS instance, to a CSV file on an external computer, and then uploading the CSV file to S3.  \n",
    "\n",
    "To copy the data from our RDS instance to the ETL server, we ran the following:\n",
    "\n",
    "```bash\n",
    "psql -U postgres --host foursquare-flask-api.cbdkozm37vkd.us-east-1.rds.amazonaws.com -c \"\\copy (Select * From states) To 'states.csv' With CSV;\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-shuttle",
   "metadata": {},
   "source": [
    "And this created a new `states.csv` file on our computer with data from our RDS instance.\n",
    "\n",
    "Then came uploading this file to our `s3` bucket.  First, we set permissions on our S3 bucket to enable public access, and set a bucket policy that gave `put` access to add additional objects to the bucket.  After the permissions were set up, we were able to upload our csv file into the bucket with the following: \n",
    "\n",
    "`aws s3 cp states.csv s3://jigsaw-sample-data/states.csv --acl public-read`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-translation",
   "metadata": {},
   "source": [
    "### Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-correspondence",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[S3 tutorial](http://boto.cloudhackers.com/en/latest/s3_tut.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-narrow",
   "metadata": {},
   "source": [
    "[Good tutorial RDS to S3](https://www.sqlshack.com/integrating-aws-s3-buckets-with-aws-rds-sql-server/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
